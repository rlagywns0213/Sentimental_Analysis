{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    base_url = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=136900&type=after&onlyActualPointYn=N&onlySpoilerPointYn=N&order=newest&page={}\"\n",
    "    reviews = []\n",
    "    for n in range(5000):\n",
    "        url = base_url.format(n+1)\n",
    "        resp = requests.get(url)\n",
    "        html = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "        score_result = html.find('div', {'class': 'score_result'})\n",
    "        score_result2 = html.find('div', {'class': 'star_score'})\n",
    "    \n",
    "    \n",
    "        lis = score_result.findAll('li')\n",
    "    \n",
    "        for li in lis :\n",
    "            review_text = li.find('p').getText()\n",
    "            star = score_result2.getText()\n",
    "            comment = review_text.strip().replace(\"\\t\",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "            star = int(star)\n",
    "            label = 0 if star < 5 else 1\n",
    "            reviews.append([label, comment])\n",
    "    train_data = [[], []]    \n",
    "    for review in reviews:\n",
    "        label = review[0]\n",
    "        comment = review[1]\n",
    "        \n",
    "        train_data[0].append(label)\n",
    "        train_data[1].append(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    base_url = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=136900&type=after&onlyActualPointYn=N&onlySpoilerPointYn=N&order=newest&page={}\"\n",
    "    reviews = []\n",
    "    for n in range(5001,6001):\n",
    "        url = base_url.format(n+1)\n",
    "        resp = requests.get(url)\n",
    "        html = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "        score_result = html.find('div', {'class': 'score_result'})\n",
    "        score_result2 = html.find('div', {'class': 'star_score'})\n",
    "    \n",
    "    \n",
    "        lis = score_result.findAll('li')\n",
    "    \n",
    "        for li in lis :\n",
    "            review_text = li.find('p').getText()\n",
    "            star = score_result2.getText()\n",
    "            comment = review_text.strip().replace(\"\\t\",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "            star = int(star)\n",
    "            label = 0 if star < 5 else 1\n",
    "            reviews.append([label, comment])\n",
    "    test_data = [[], []]    \n",
    "    for review in reviews:\n",
    "        label = review[0]\n",
    "        comment = review[1]\n",
    "        \n",
    "        test_data[0].append(label)\n",
    "        test_data[1].append(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm은 정규화, stem은 근어로 표시하기를 나타냄\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "if os.path.isfile('train_docs.json'):\n",
    "    with open('train_docs.json', encoding ='UTF-8') as f:\n",
    "        train_docs = json.load(f)\n",
    "    with open('test_docs.json',encoding='UTF-8') as f:\n",
    "        test_docs = json.load(f)\n",
    "else:\n",
    "    train_docs = []\n",
    "    test_docs = []\n",
    "    for i in range(0,len(train_data[0])):\n",
    "        train_docs.append([tokenize(train_data[1][i]), train_data[0][i]])\n",
    "    for i in range(0,len(test_data[0])):\n",
    "        test_docs.append([tokenize(test_data[1][i]), test_data[0][i]])\n",
    "    with open('train_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(train_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "    with open('test_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(test_docs, make_file, ensure_ascii=False, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens, name='NMSC')\n",
    "\n",
    "# 전체 토큰의 개수\n",
    "print(len(text.tokens))\n",
    "\n",
    "# 중복을 제외한 토큰의 개수\n",
    "print(len(set(text.tokens)))            \n",
    "\n",
    "# 출현 빈도가 높은 상위 토큰 10개\n",
    "print(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "font_fname = 'c:/windows/fonts/gulim.ttc'\n",
    "font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "text.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = [f[0] for f in text.vocab().most_common(3000)]\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "train_x = [term_frequency(d) for d, _ in train_docs]\n",
    "test_x = [term_frequency(d) for d, _ in test_docs]\n",
    "train_y = [c for _, c in train_docs]\n",
    "test_y = [c for _, c in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델링을 하기 위해 리스트로 되어 있는 데이터 형식을 array로 바꿔주고 dtype도 실수로 바꿈\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.asarray(train_x).astype('float32')\n",
    "x_test = np.asarray(test_x).astype('float32')\n",
    "\n",
    "y_train = np.asarray(train_y).astype('float32')\n",
    "y_test = np.asarray(test_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "\n",
    "# 모델 구조 정의하기\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(3000,))) #500개로 Vectorization 해주었기 때문에 input_shape 500으로 입력\n",
    "model.add(layers.Dense(64, activation='relu')) #ReLU 활성화함수 채택\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#분류(classification)을 위해 softmax 함수 사용 실수를 확률의 값(0~1사이)\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss=losses.binary_crossentropy,\n",
    "             metrics=[metrics.binary_accuracy])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=512) #512개에 한 번씩 업데이터 실행\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos_neg(review):\n",
    "    token = tokenize(review) #토큰화\n",
    "    tf = term_frequency(token) #토큰화된 단어를 이용해서 가장 많이 등장하는 단어와의 빈도수 체크\n",
    "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data)) #새로운 데이터를 받으면 결과 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\\n\".format(review, score * 100))\n",
    "    else:\n",
    "        print(\"[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\\n\".format(review, (1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_pos_neg(\"올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요.\")\n",
    "predict_pos_neg(\"배경 음악이 영화의 분위기랑 너무 안 맞았습니다. 몰입에 방해가 됩니다.\")\n",
    "predict_pos_neg(\"주연 배우가 신인인데 연기를 진짜 잘 하네요. 몰입감 ㅎㄷㄷ\")\n",
    "predict_pos_neg(\"믿고 보는 감독이지만 이번에는 아니네요\")\n",
    "predict_pos_neg(\"주연배우 때문에 봤어요\")\n",
    "predict_pos_neg(\"다신 안볼꺼야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
